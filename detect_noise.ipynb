{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "TRAIN_DIR = 'data/train'\n",
    "VAL_DIR = 'data/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=22):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку последовательности в батче для обучения могут быть разного размера, добавим padding до максимальной длины спектрограммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(records):\n",
    "    max_length = max(record.shape[0] for record in records)\n",
    "    embedding_dim = records[0].shape[1]\n",
    "    X = np.empty(shape=(len(records), max_length, embedding_dim))\n",
    "    for i, record in enumerate(records):\n",
    "        if len(record) < max_length:\n",
    "            max_offset = max_length - len(record)\n",
    "            offset = np.random.randint(max_offset) if max_offset else max_offset\n",
    "            record = np.pad(record, ((max_offset, 0), (0, 0)), \"constant\")\n",
    "        X[i,] = record\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На вход сети поочередно подаем батч с чистой речью, затем зашумленный батч"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(directory):\n",
    "    clean_path = os.path.join(directory, 'clean')\n",
    "    noisy_path = os.path.join(directory, 'noisy')\n",
    "    for subdir in os.listdir(clean_path):\n",
    "        clean_samples = [np.load(f) for f in glob.glob(os.path.join(clean_path, '{}/*.npy'.format(subdir)))]\n",
    "        clean_lables = np.zeros(len(clean_samples))\n",
    "        yield clean_samples, clean_lables\n",
    "        noisy_samples = [np.load(f) for f in glob.glob(os.path.join(noisy_path, '{}/*.npy'.format(subdir)))]\n",
    "        noisy_lables = np.ones(len(noisy_samples))\n",
    "        yield noisy_samples, noisy_lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку mel-спектрограмма является последовательностью данных во времени, для решения поставленной задачи применим рекуррентную архитектуру сети, а именно LSTM, с fc-слоем в конце для классификации. Чтобы избежать переобучения добавим dropout-слой. Также рекуррентная архитектура упрощает работу с последовательностями различной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dropout_layer = torch.nn.Dropout(p=0.2)\n",
    "        self.hidden2out = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        outputs, (ht, ct) = self.lstm(sentence.view(sentence.shape[1], sentence.shape[0], self.embedding_dim))\n",
    "        output = self.dropout_layer(ht[-1])\n",
    "        output = self.hidden2out(output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, num_epochs, scheduler=None):\n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for batch, lables in get_batch(TRAIN_DIR):\n",
    "            optimizer.zero_grad()\n",
    "            x_gpu = torch.tensor(prepare_data(batch), dtype=torch.float32).cuda()\n",
    "            y_gpu = torch.tensor(lables, dtype=torch.float32).view(len(lables), 1).cuda()\n",
    "            prediction = model(x_gpu)\n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            loss_value.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            indices = (prediction > THRESHOLD).float()\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            loss_accum += loss_value\n",
    "                \n",
    "            total_samples += len(batch)\n",
    "            \n",
    "        ave_loss = loss_accum / total_samples\n",
    "        loss_history.append(float(ave_loss))\n",
    "        \n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        train_history.append(train_accuracy)\n",
    "        \n",
    "        val_accuracy = compute_accuracy(model)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"Average loss: {:.5}, Train accuracy: {:.5}, Val accuracy: {:.5}\".format(\n",
    "            ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    return loss_history, train_history, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, ):\n",
    "    model.eval()\n",
    "    \n",
    "    accuracy = 0\n",
    "    correct_samples = 0\n",
    "    total_samples = 0\n",
    "    for batch, lables in get_batch(VAL_DIR):\n",
    "        x_gpu = torch.tensor(prepare_data(batch), dtype=torch.float32).cuda()\n",
    "        y_gpu = torch.tensor(lables, dtype=torch.float32).view(len(lables), 1).cuda()\n",
    "        prediction = model(x_gpu)\n",
    "\n",
    "        indices = (prediction > THRESHOLD).float()\n",
    "        correct_samples += torch.sum(indices == y_gpu)\n",
    "            \n",
    "        total_samples += len(batch)\n",
    "    if total_samples:\n",
    "        accuracy = correct_samples.data.float() / total_samples\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функцию тестирования нужно передать модель и mel-спектрограмму. В ответ возвращается 0 или 1, 1 - значит соответствующая спектрограмма содержит шум. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, mel):\n",
    "    x_gpu = torch.tensor(mel, dtype=torch.float32).cuda()\n",
    "    prediction = model(x_gpu.unsqueeze(0))\n",
    "    return (prediction > THRESHOLD).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры модели и оптимизации подбирались по сетке до достижения приемлемого качества классификации, с ошибкой ~0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20\n",
    "EMBEDDING_DIM = 80\n",
    "HIDDEN_DIM = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.017475, Train accuracy: 0.90317, Val accuracy: 0.92925\n",
      "Average loss: 0.013535, Train accuracy: 0.92067, Val accuracy: 0.94925\n",
      "Average loss: 0.011492, Train accuracy: 0.94012, Val accuracy: 0.969\n",
      "Average loss: 0.0082945, Train accuracy: 0.95388, Val accuracy: 0.964\n",
      "Average loss: 0.0082619, Train accuracy: 0.9555, Val accuracy: 0.97375\n",
      "Average loss: 0.0061697, Train accuracy: 0.97537, Val accuracy: 0.98525\n",
      "Average loss: 0.0044598, Train accuracy: 0.98279, Val accuracy: 0.9855\n",
      "Average loss: 0.004288, Train accuracy: 0.98746, Val accuracy: 0.99175\n",
      "Average loss: 0.0035776, Train accuracy: 0.99042, Val accuracy: 0.98775\n",
      "Average loss: 0.0045591, Train accuracy: 0.98337, Val accuracy: 0.99025\n",
      "Average loss: 0.0029836, Train accuracy: 0.99179, Val accuracy: 0.9965\n",
      "Average loss: 0.0020312, Train accuracy: 0.99462, Val accuracy: 0.9895\n",
      "Average loss: 0.0027822, Train accuracy: 0.98913, Val accuracy: 0.99325\n",
      "Average loss: 0.0016414, Train accuracy: 0.99596, Val accuracy: 0.9935\n",
      "Average loss: 0.0021469, Train accuracy: 0.99371, Val accuracy: 0.99225\n",
      "Average loss: 0.001906, Train accuracy: 0.99442, Val accuracy: 0.99425\n",
      "Average loss: 0.0016084, Train accuracy: 0.99604, Val accuracy: 0.99475\n",
      "Average loss: 0.0016751, Train accuracy: 0.99525, Val accuracy: 0.99375\n",
      "Average loss: 0.0018953, Train accuracy: 0.99425, Val accuracy: 0.994\n",
      "Average loss: 0.0017031, Train accuracy: 0.995, Val accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 1).cuda()\n",
    "loss_function = torch.nn.BCELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "loss_history, train_history, val_history = train_model(model, loss_function, optimizer, NUM_EPOCHS, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_mel = np.load(os.path.join(VAL_DIR, 'clean/8897/8897_294717_8897-294717-0001.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_test_mel = np.load(os.path.join(VAL_DIR, 'noisy/8897/8897_294717_8897-294717-0001.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, clean_test_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model, noisy_test_mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
